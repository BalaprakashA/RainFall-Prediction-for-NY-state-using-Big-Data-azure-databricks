{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980e471f-41ca-41c0-a7bf-3c9fdcac9104",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db4e989-36df-4259-8edc-025939adff9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+--------------+-------------+----------+----+--------+-----------+\n|station_id|state_code|station_list_no|network_div_no|element_units|      date|hour|rainfall|      state|\n+----------+----------+---------------+--------------+-------------+----------+----+--------+-----------+\n|  22079200|        22|            792|             0|           HI|1955-12-03|2500|     297|Mississippi|\n|  22079200|        22|            792|             0|           HI|1955-12-04|2500|      22|Mississippi|\n|  22079200|        22|            792|             0|           HI|1955-12-05|2500|       9|Mississippi|\n|  22079200|        22|            792|             0|           HI|1955-12-06|2500|      26|Mississippi|\n|  22079200|        22|            792|             0|           HI|1955-12-18|2500|      83|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-01-01|2500|       0|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-01-03|2500|      11|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-01-18|2500|      40|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-01-22|2500|     260|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-01-23|2500|      44|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-01-30|2500|       5|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-01|2500|       5|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-02|2500|      11|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-03|2500|       5|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-04|2500|      10|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-05|2500|      37|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-06|2500|       8|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-08|2500|      84|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-09|2500|      17|Mississippi|\n|  22079200|        22|            792|             0|           HI|1956-02-10|2500|      61|Mississippi|\n+----------+----------+---------------+--------------+-------------+----------+----+--------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"HiveQuery\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "query = \"SELECT * FROM hive_metastore.default.daily_rainfall_flags_removed\"\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520f5c75-3787-45cf-91eb-5019f8a310e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"month\", F.month(\"date\")).withColumn(\"year\", F.year(\"date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e736bc-fd6d-4349-a728-963e7a9579d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_remove = [\"station_id\", \"state_code\", \"station_list_no\", \"network_div_no\", \"element_units\",\"hour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c2207c-0c6c-4b0d-8fb9-48bf0a7d561c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-----+----+\n|      date|rainfall|      state|month|year|\n+----------+--------+-----------+-----+----+\n|1955-12-03|     297|Mississippi|   12|1955|\n|1955-12-04|      22|Mississippi|   12|1955|\n|1955-12-05|       9|Mississippi|   12|1955|\n|1955-12-06|      26|Mississippi|   12|1955|\n|1955-12-18|      83|Mississippi|   12|1955|\n|1956-01-01|       0|Mississippi|    1|1956|\n|1956-01-03|      11|Mississippi|    1|1956|\n|1956-01-18|      40|Mississippi|    1|1956|\n|1956-01-22|     260|Mississippi|    1|1956|\n|1956-01-23|      44|Mississippi|    1|1956|\n|1956-01-30|       5|Mississippi|    1|1956|\n|1956-02-01|       5|Mississippi|    2|1956|\n|1956-02-02|      11|Mississippi|    2|1956|\n|1956-02-03|       5|Mississippi|    2|1956|\n|1956-02-04|      10|Mississippi|    2|1956|\n|1956-02-05|      37|Mississippi|    2|1956|\n|1956-02-06|       8|Mississippi|    2|1956|\n|1956-02-08|      84|Mississippi|    2|1956|\n|1956-02-09|      17|Mississippi|    2|1956|\n|1956-02-10|      61|Mississippi|    2|1956|\n+----------+--------+-----------+-----+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Remove the specified columns\n",
    "df= df.drop(*columns_to_remove)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4625e6be-37cd-4ccf-a409-c740c6c6e0d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- date: date (nullable = true)\n |-- rainfall: long (nullable = true)\n |-- state: string (nullable = true)\n |-- month: integer (nullable = true)\n |-- year: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625a67ee-7aa3-4d26-8fb1-1d16879b41ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum rainfall value: 0\nMaximum rainfall value: 1320\n"
     ]
    }
   ],
   "source": [
    "# Min and Max values of the 'rainfall' column\n",
    "min_rainfall = df.agg({\"rainfall\": \"min\"}).collect()[0][0]\n",
    "max_rainfall = df.agg({\"rainfall\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Minimum rainfall value: {min_rainfall}\")\n",
    "print(f\"Maximum rainfall value: {max_rainfall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a7b6b1-dd0e-4b70-991b-12380d38818f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test & Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e709b5f-2674-4077-92bc-e4537b1d5665",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 336.59683375558586\nR-squared (R2): 0.861730301521382\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Assuming you have already loaded and preprocessed the data into a DataFrame named 'df'\n",
    "\n",
    "# Prepare the data for model training\n",
    "vector_assembler = VectorAssembler(inputCols=[\"rainfall\", \"year\"], outputCol=\"features\")\n",
    "df_model = vector_assembler.transform(df)\n",
    "\n",
    "# Split the data into training and test sets (80% for training, 20% for testing)\n",
    "train_data, test_data = df_model.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the Random Forest regression model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rainfall\")\n",
    "\n",
    "# Train the Random Forest model on the training data\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model's performance using Mean Squared Error (MSE)\n",
    "evaluator = RegressionEvaluator(labelCol=\"rainfall\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Calculate the R-squared (R2) value\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f1d0fd-9280-4875-8650-f0d3a5ef5ee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Example of fine-tuning the Random Forest model's hyperparameters\n",
    "\n",
    "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# # Create a ParamGrid for hyperparameter tuning\n",
    "# param_grid = (ParamGridBuilder()\n",
    "#               .addGrid(rf.numTrees, [10, 20, 30])\n",
    "#               .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "#               .build())\n",
    "\n",
    "# # Initialize the CrossValidator with the Random Forest model and ParamGrid\n",
    "# crossval = CrossValidator(estimator=rf,\n",
    "#                           estimatorParamMaps=param_grid,\n",
    "#                           evaluator=evaluator,\n",
    "#                           numFolds=3)  # Number of folds for cross-validation\n",
    "\n",
    "# # Run cross-validation to find the best hyperparameters\n",
    "# cv_model = crossval.fit(train_data)\n",
    "\n",
    "# # Get the best model with the tuned hyperparameters\n",
    "# best_model = cv_model.bestModel\n",
    "\n",
    "# # Make predictions on the test data using the best model\n",
    "# predictions_tuned = best_model.transform(test_data)\n",
    "\n",
    "# # Evaluate the best model's performance on the test data\n",
    "# mse_test_tuned = evaluator.evaluate(predictions_tuned)\n",
    "# r2_test_tuned = evaluator.evaluate(predictions_tuned, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# print(\"Mean Squared Error (MSE) on Test Data (Tuned Model):\", mse_test_tuned)\n",
    "# print(\"R-squared (R2) on Test Data (Tuned Model):\", r2_test_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364a541f-ee7e-4e5f-84ed-0dfb3b8ee8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) for New Jersey: 22.288775676779444\nR-squared (R2) for New Jersey: 0.9504686822515916\nMean Squared Error (MSE) for New York: 167.16229902092638\nR-squared (R2) for New York: 0.8927971477113349\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Assuming you have already loaded and preprocessed the data into a DataFrame named 'df'\n",
    "\n",
    "# Create a SparkSession (if not already created)\n",
    "spark = SparkSession.builder.appName(\"RainfallPrediction\").getOrCreate()\n",
    "\n",
    "# Filter the data for New Jersey and New York states\n",
    "df_nj = df.filter(df[\"state\"] == \"New Jersey\")\n",
    "df_ny = df.filter(df[\"state\"] == \"New York\")\n",
    "\n",
    "# Prepare the data for model training for New Jersey\n",
    "vector_assembler_nj = VectorAssembler(inputCols=[\"rainfall\", \"year\"], outputCol=\"features\")\n",
    "df_model_nj = vector_assembler_nj.transform(df_nj)\n",
    "\n",
    "# Prepare the data for model training for New York\n",
    "vector_assembler_ny = VectorAssembler(inputCols=[\"rainfall\", \"year\"], outputCol=\"features\")\n",
    "df_model_ny = vector_assembler_ny.transform(df_ny)\n",
    "\n",
    "# Split the data into training and test sets for New Jersey (80% for training, 20% for testing)\n",
    "train_data_nj, test_data_nj = df_model_nj.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Split the data into training and test sets for New York (80% for training, 20% for testing)\n",
    "train_data_ny, test_data_ny = df_model_ny.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the Random Forest regression model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rainfall\")\n",
    "\n",
    "# Train the Random Forest model on the training data for New Jersey\n",
    "model_nj = rf.fit(train_data_nj)\n",
    "\n",
    "# Train the Random Forest model on the training data for New York\n",
    "model_ny = rf.fit(train_data_ny)\n",
    "\n",
    "# Make predictions on the test data for New Jersey\n",
    "predictions_nj = model_nj.transform(test_data_nj)\n",
    "\n",
    "# Make predictions on the test data for New York\n",
    "predictions_ny = model_ny.transform(test_data_ny)\n",
    "\n",
    "# Evaluate the models' performance using Mean Squared Error (MSE) for New Jersey\n",
    "mse_nj = evaluator.evaluate(predictions_nj)\n",
    "print(\"Mean Squared Error (MSE) for New Jersey:\", mse_nj)\n",
    "\n",
    "# Calculate the R-squared (R2) value for New Jersey\n",
    "r2_nj = evaluator.evaluate(predictions_nj, {evaluator.metricName: \"r2\"})\n",
    "print(\"R-squared (R2) for New Jersey:\", r2_nj)\n",
    "\n",
    "# Evaluate the models' performance using Mean Squared Error (MSE) for New York\n",
    "mse_ny = evaluator.evaluate(predictions_ny)\n",
    "print(\"Mean Squared Error (MSE) for New York:\", mse_ny)\n",
    "\n",
    "# Calculate the R-squared (R2) value for New York\n",
    "r2_ny = evaluator.evaluate(predictions_ny, {evaluator.metricName: \"r2\"})\n",
    "print(\"R-squared (R2) for New York:\", r2_ny)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b30b9a6b-57aa-4471-8444-8bc9032afaee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n|year|month|        prediction|\n+----+-----+------------------+\n|2012|    1|22.364033760841412|\n|2012|    2|21.169451768323132|\n|2012|    3| 27.86094301646122|\n|2012|    4|24.184208763699555|\n|2012|    5|25.057021730852135|\n|2012|    6| 29.30191628609786|\n|2012|    7|29.555952657970558|\n|2012|    8| 28.02771304764036|\n|2012|    9|27.720986862323553|\n|2012|   10|27.266379905098383|\n|2012|   11|27.391609524625817|\n|2012|   12| 27.45301069255609|\n+----+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_ny = df.filter(df[\"state\"] == \"New York\")\n",
    "\n",
    "\n",
    "vector_assembler_ny = VectorAssembler(inputCols=[\"year\", \"month\"], outputCol=\"features\")\n",
    "df_model_ny = vector_assembler_ny.transform(df_ny)\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rainfall\")\n",
    "\n",
    "\n",
    "model_ny = rf.fit(df_model_ny)\n",
    "\n",
    "df_2012 = spark.range(1, 13).selectExpr(\"id as month\")\n",
    "df_2012 = df_2012.withColumn(\"year\", lit(2012))\n",
    "\n",
    "df_model_2012 = vector_assembler_ny.transform(df_2012)\n",
    "\n",
    "# Make predictions for the data of New York in the year 2012\n",
    "predictions_2012 = model_ny.transform(df_model_2012)\n",
    "\n",
    "# Show the predictions for New York in the year 2012\n",
    "predictions_2012.select(\"year\", \"month\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31bebe0-f74c-4494-b3d8-5efdfe77032f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "RainFall Prediction for NY state",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
